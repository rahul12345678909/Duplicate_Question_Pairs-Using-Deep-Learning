{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n",
      "/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n",
      "/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n",
      "/kaggle/input/quora-question-pairs/test.csv\n",
      "/kaggle/input/quora-question-pairs/test.csv.zip\n",
      "/kaggle/input/quora-question-pairs/sample_submission.csv.zip\n",
      "/kaggle/input/quora-question-pairs/train.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/quora-question-pairs/train.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above dataset, we are given question pairs and a 'is_duplicate' column which tells us whether the question pairs are duplicate of each other or not. We'll train our model over this train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('../input/quora-question-pairs/test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id                                          question1  \\\n",
       "0       0  How does the Surface Pro himself 4 compare wit...   \n",
       "1       1  Should I have a hair transplant at age 24? How...   \n",
       "2       2  What but is the best way to send money from Ch...   \n",
       "3       3                        Which food not emulsifiers?   \n",
       "4       4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our test data. We'll calculate the similarity between these question pairs to get the idea whether the are duplicates or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:,:5].values\n",
    "Y_train = df.iloc[:,5:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testq1 = test_data.iloc[:400001,1:2].values\n",
    "X_testq2 = test_data.iloc[:400001, 2:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = X_train[:,3]\n",
    "s2 = X_train[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    tokens = []\n",
    "    tokens = [word_tokenize(str(sentence)) for sentence in s]\n",
    "\n",
    "    rm1 = []\n",
    "    for w in tokens:\n",
    "        sm = re.sub('[^A-Za-z]',' ', str(w))\n",
    "        x = re.split(\"\\s\", sm)\n",
    "        rm1.append(x)\n",
    "        \n",
    "    return rm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(s):\n",
    "    #Removing whitespaces    \n",
    "    for sent in s:\n",
    "        while '' in sent:\n",
    "            sent.remove('')\n",
    "\n",
    "    # Lowercasing\n",
    "    low = []\n",
    "    for i in s:\n",
    "        i = [x.lower() for x in i]\n",
    "        low.append(i)\n",
    "        \n",
    "    return low\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(s):\n",
    "    lemma = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for doc in s:\n",
    "        tokens = [wnl.lemmatize(w) for w in doc]\n",
    "        lemma.append(tokens)\n",
    "\n",
    "    # Removing Stopwords\n",
    "    filter_words = []\n",
    "    Stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    #ab = spell('nd')\n",
    "    for sent in lemma:\n",
    "        tokens = [w for w in sent if w not in Stopwords]\n",
    "        filter_words.append(tokens)\n",
    "\n",
    "    space = ' ' \n",
    "    sentences = []\n",
    "    for sentence in filter_words:\n",
    "        sentences.append(space.join(sentence))\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent1 = tokenize(s1)\n",
    "# sent2 = tokenize(s2)\n",
    "# q1 = lower_case(sent1)\n",
    "# q2 = lower_case(sent2)\n",
    "# listq1 = lemmatize(q1)\n",
    "# listq2 = lemmatize(q2)\n",
    "# sent1_t = tokenize(X_test_q1)\n",
    "# sent2_t = tokenize(X_test_q2)\n",
    "# q1_t = lower_case(sent1_t)\n",
    "# q2_t = lower_case(sent2_t)\n",
    "# listq1 = lemmatize(q1_t)\n",
    "# listq2 = lemmatize(q2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras text preprocessing\n",
    "\n",
    "When approaching a NLP problem, either you can perform all the above mentioned steps in that order or you can use Keras' Tokenizer class to perform tokenization. In this project, I have tried out Keras' Tokenizer class and it also works pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX_NB_WORDS is a constant which indicates the maximum number of words that should be present.\n",
    "Next, we fit out Tokenizer on all the questions in column 'question1' and 'question2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(df['question1'].values.astype(str))+list(df['question2'].values.astype(str)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and sequencing\n",
    "Now, we convert all the questions in column 'question1' and 'question2', of both train and test set, into sequences, i.e, in the form of numbers since machine can only process numbers and not texts. \n",
    "We define the maximum length of each question and the questions which contain less than the required length are padded with zeros to make the length of the sentence equal to the mentioned length.\n",
    "\n",
    "For example, maxlen = 5\n",
    "\n",
    "sent1 = ['I', 'love','apples']\n",
    "\n",
    "After sequencing,\n",
    "\n",
    "sent1 = [1,2,3]\n",
    "\n",
    "After padding,\n",
    "\n",
    "sent1 = [1,2,3,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_q1 = tokenizer.texts_to_sequences(np.array(listq1))\n",
    "X_train_q1 = tokenizer.texts_to_sequences(df['question1'].values.astype(str))\n",
    "X_train_q1 = pad_sequences(X_train_q1, maxlen = 30, padding='post')\n",
    "\n",
    "# X_train_q2 = tokenizer.texts_to_sequences(np.array(listq2))\n",
    "X_train_q2 = tokenizer.texts_to_sequences(df['question2'].values.astype(str))\n",
    "X_train_q2 = pad_sequences(X_train_q2, maxlen = 30, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_q1 = tokenizer.texts_to_sequences(X_testq1.ravel())\n",
    "X_test_q1 = pad_sequences(X_test_q1,maxlen = 30, padding='post')\n",
    "\n",
    "X_test_q2 = tokenizer.texts_to_sequences(X_testq2.astype(str).ravel())\n",
    "X_test_q2 = pad_sequences(X_test_q2, maxlen = 30, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Glove word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors) is a model for distributed representations. The model is an unsupervised learning algorithm for obtaining vector representation for words. This is taken care of by mapping words into meaningful space where the distance between the words is related to semantic similarity. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], 'float32')\n",
    "        embedding_index[word] = vectors\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index)+1, 200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM coding begins !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing Gradients Problem\n",
    "The problem that vanilla RNNs face is vanishing gradients problem. Vanilla RNNS are incapable of handling long range dependencies. By long range dependencies, we mean that if we are trying to generate a word based on previous inputs then if the gap between the word to be generated and the previous input considered is small, RNNs will do a great job and able to predict the next word but if the gap is large, RNNs fail to perform. This is called the vanishing gradients problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Q1\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "model_q1 = tf.keras.Sequential()\n",
    "model_q1.add(Embedding(input_dim = len(word_index)+1,\n",
    "                       output_dim = 200,\n",
    "                      weights = [embedding_matrix],\n",
    "                      input_length = 30))\n",
    "model_q1.add(LSTM(128, activation = 'tanh', return_sequences = True))\n",
    "model_q1.add(Dropout(0.2))\n",
    "model_q1.add(LSTM(128, return_sequences = True))\n",
    "model_q1.add(LSTM(128))\n",
    "model_q1.add(Dense(60, activation = 'tanh'))\n",
    "model_q1.add(Dense(2, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for Q2\n",
    "model_q2 = tf.keras.Sequential()\n",
    "model_q2.add(Embedding(input_dim = len(word_index)+1,\n",
    "                       output_dim = 200,\n",
    "                      weights = [embedding_matrix],\n",
    "                      input_length = 30))\n",
    "model_q2.add(LSTM(128, activation = 'tanh', return_sequences = True))\n",
    "model_q2.add(Dropout(0.2))\n",
    "model_q2.add(LSTM(128, return_sequences = True))\n",
    "model_q2.add(LSTM(128))\n",
    "model_q2.add(Dense(60, activation = 'tanh'))\n",
    "model_q2.add(Dense(2, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the output of the two models,i.e, model_q1 and model_q2\n",
    "mergedOut = Multiply()([model_q1.output, model_q2.output])\n",
    "\n",
    "mergedOut = Flatten()(mergedOut)\n",
    "mergedOut = Dense(100, activation = 'relu')(mergedOut)\n",
    "mergedOut = Dropout(0.2)(mergedOut)\n",
    "mergedOut = Dense(50, activation = 'relu')(mergedOut)\n",
    "mergedOut = Dropout(0.2)(mergedOut)\n",
    "mergedOut = Dense(2, activation = 'sigmoid')(mergedOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 404290 samples\n",
      "Epoch 1/10\n",
      "404290/404290 [==============================] - 136s 336us/sample - loss: 0.6267 - accuracy: 0.6508\n",
      "Epoch 2/10\n",
      "404290/404290 [==============================] - 126s 311us/sample - loss: 0.5385 - accuracy: 0.7315\n",
      "Epoch 3/10\n",
      "404290/404290 [==============================] - 126s 313us/sample - loss: 0.5023 - accuracy: 0.7551\n",
      "Epoch 4/10\n",
      "404290/404290 [==============================] - 127s 313us/sample - loss: 0.4736 - accuracy: 0.7703\n",
      "Epoch 5/10\n",
      "404290/404290 [==============================] - 127s 314us/sample - loss: 0.4478 - accuracy: 0.7837\n",
      "Epoch 6/10\n",
      "404290/404290 [==============================] - 127s 313us/sample - loss: 0.4241 - accuracy: 0.7959\n",
      "Epoch 7/10\n",
      "404290/404290 [==============================] - 126s 313us/sample - loss: 0.4028 - accuracy: 0.8070\n",
      "Epoch 8/10\n",
      "404290/404290 [==============================] - 127s 313us/sample - loss: 0.3836 - accuracy: 0.8172\n",
      "Epoch 9/10\n",
      "404290/404290 [==============================] - 126s 312us/sample - loss: 0.3647 - accuracy: 0.8277\n",
      "Epoch 10/10\n",
      "404290/404290 [==============================] - 127s 313us/sample - loss: 0.3469 - accuracy: 0.8370\n"
     ]
    }
   ],
   "source": [
    "new_model = Model([model_q1.input, model_q2.input], mergedOut)\n",
    "new_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "history = new_model.fit([X_train_q1,X_train_q2],Y_train, batch_size = 2000, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400001/400001 [==============================] - 11s 27us/sample\n",
      "400001/400001 [==============================] - 9s 24us/sample\n"
     ]
    }
   ],
   "source": [
    "y_pred = new_model.predict([X_test_q1, X_test_q2], batch_size=2000, verbose=1)\n",
    "y_pred += new_model.predict([X_test_q1, X_test_q2], batch_size=2000, verbose=1)\n",
    "y_pred /= 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
